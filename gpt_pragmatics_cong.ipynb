{"cells":[{"cell_type":"markdown","metadata":{"id":"KPaQXEI09ePI"},"source":["# import library and api_key from openAI"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":346,"status":"ok","timestamp":1639672623177,"user":{"displayName":"Amir Hossein Nikzad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEw0V2vxgWC1JWG4Ek_RNzYSjpn67NjOGsaeiyhQ=s64","userId":"08818856783092278569"},"user_tz":-210},"id":"AcUYGrVhqOTy"},"outputs":[],"source":["# Author: Yan CONG\n","# import libraries\n","import pandas as pd\n","import numpy as np\n","import math\n","import os\n","import re"]},{"cell_type":"markdown","metadata":{},"source":["# GPT3 parameters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class GPT:\n","    \"\"\"The main class for a user to interface with the OpenAI API.\n","\n","    A user can add examples and set parameters of the API request.\n","    \"\"\"\n","    def __init__(self,\n","                 engine='text-davinci-001', \n","                 temperature=0.5,\n","                 max_tokens=100,\n","                 logprobs=5, \n","                 input_prefix=\"input: \",\n","                 input_suffix=\"\\n\",\n","                 output_prefix=\"output: \",\n","                 output_suffix=\"\\n\\n\",\n","                 append_output_prefix_to_query=False):\n","        self.examples = {}\n","        self.engine = engine\n","        self.temperature = temperature\n","        self.max_tokens = max_tokens\n","        self.input_prefix = input_prefix\n","        self.input_suffix = input_suffix\n","        self.output_prefix = output_prefix\n","        self.output_suffix = output_suffix\n","        self.append_output_prefix_to_query = append_output_prefix_to_query\n","        self.stop = (output_suffix + input_prefix).strip()\n","\n","    def add_example(self, ex):\n","        \"\"\"Adds an example to the object.\n","\n","        Example must be an instance of the Example class.\n","        \"\"\"\n","        assert isinstance(ex, Example), \"Please create an Example object.\"\n","        self.examples[ex.get_id()] = ex\n","\n","    def delete_example(self, id):\n","        \"\"\"Delete example with the specific id.\"\"\"\n","        if id in self.examples:\n","            del self.examples[id]\n","\n","    def get_example(self, id):\n","        \"\"\"Get a single example.\"\"\"\n","        return self.examples.get(id, None)\n","\n","    def get_all_examples(self):\n","        \"\"\"Returns all examples as a list of dicts.\"\"\"\n","        return {k: v.as_dict() for k, v in self.examples.items()}\n","\n","    def get_prime_text(self):\n","        \"\"\"Formats all examples to prime the model.\"\"\"\n","        return \"\".join(\n","            [self.format_example(ex) for ex in self.examples.values()])\n","\n","    def get_engine(self):\n","        \"\"\"Returns the engine specified for the API.\"\"\"\n","        return self.engine\n","\n","    def get_temperature(self):\n","        \"\"\"Returns the temperature specified for the API.\"\"\"\n","        return self.temperature\n","\n","    def get_max_tokens(self):\n","        \"\"\"Returns the max tokens specified for the API.\"\"\"\n","        return self.max_tokens\n","\n","    def craft_query(self, prompt):\n","        \"\"\"Creates the query for the API request.\"\"\"\n","        q = self.get_prime_text(\n","        ) + self.input_prefix + prompt + self.input_suffix\n","        if self.append_output_prefix_to_query:\n","            q = q + self.output_prefix\n","\n","        return q \n","    \n","    \n","    def submit_request(self, prompt):\n","        \"\"\"Calls the OpenAI API with the specified parameters.\"\"\"\n","        response = openai.Completion.create(engine=self.get_engine(),\n","                                            prompt=self.craft_query(prompt),\n","                                            max_tokens=self.get_max_tokens(),\n","                                            temperature=self.get_temperature(),\n","                                            logprobs=3, #yan added otherwise default null\n","                                            top_p=1,\n","                                            n=1,\n","                                            stream=False,\n","                                            stop=self.stop)\n","        return response\n","\n","    def get_top_reply(self, prompt):\n","        \"\"\"Obtains the best result as returned by the API.\"\"\"\n","        response = self.submit_request(prompt)\n","        return response['choices'][0]['text']\n","\n","    #yan added\n","    def get_logprobs(self, prompt):\n","        \"\"\"Obtains the logprobs as returned by the API.\"\"\"\n","        response = self.submit_request(prompt)\n","        return response['choices'][0]['logprobs']      \n","\n","    def format_example(self, ex):\n","        \"\"\"Formats the input, output pair.\"\"\"\n","        return self.input_prefix + ex.get_input(\n","        ) + self.input_suffix + self.output_prefix + ex.get_output(\n","        ) + self.output_suffix"]},{"cell_type":"markdown","metadata":{},"source":["# Process GPT3 outputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def logprobs_to_percent(x):\n","    for k,v in x.items():\n","        p = 100*np.e**v\n","        x[k] = p\n","    return x\n"]},{"cell_type":"markdown","metadata":{},"source":["# Starting here: call func and class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# the GPT class parameters setting will get updated here\n","gpt = GPT(engine=\"text-davinci-001\", #old davinci\n","          temperature=0, #if we increase the temperature, it takes non-optimal answers. \n","          logprobs=3, \n","          max_tokens=30) #The maximum number of tokens to generate in the completion"]},{"cell_type":"markdown","metadata":{},"source":["# Run datasets"]},{"cell_type":"markdown","metadata":{},"source":["## Run scalar implicature cw quantifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in df.index:\n","    prompt = df['cw'][i] + df['post_cw'][i].strip('.') + \";\"\n","    r = gpt.submit_request(prompt)\n","    pilot = pd.DataFrame(r['choices'][0]['logprobs'])\n","\n","    pilot[\"%_token_logprobs\"] = pilot[\"token_logprobs\"].apply(lambda x: 100*np.e**x)\n","    pilot[\"%_top_logprobs\"] = pilot[\"top_logprobs\"].apply(lambda x: logprobs_to_percent(x))\n","\n","    pilot['cw'] = df['cw'][i]\n","    pilot['post_cw'] = df['post_cw'][i]\n","\n","    pilot.to_csv(str(i) + '.csv')"]},{"cell_type":"markdown","metadata":{},"source":["# Prep for data analysis"]},{"cell_type":"markdown","metadata":{},"source":["## Prep Manner implicature stats"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = df[df['mean_%_token_logprobs']>0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['condition'] = ''\n","df['prediction'] = ''\n","for i in df.index:\n","    if i == 0 or i % 2 == 0:\n","        df['condition'][i] = 'experimental'\n","        df['prediction'][i] = 'good'\n","    else:\n","        df['condition'][i] = 'baseline'\n","        df['prediction'][i] = 'bad'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['accuracy'] = ''\n","for i in df.index:\n","    if i%2 == 0:\n","        try:\n","            if df['mean_%_token_logprobs'][i] >= df['mean_%_token_logprobs'][i+1]:\n","                df['accuracy'][i] = '1'\n","            else:\n","                df['accuracy'][i] = '0'\n","        except:\n","            pass\n","    else:\n","        df['accuracy'][i] = 'na'"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"3_transcripts_to_aggregates.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
